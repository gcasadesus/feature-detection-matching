{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Absolute Localization\n",
    "\n",
    "This notebook helps visualize and debug the absolute localization evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/home/guillemc/dev/LuPNT-private\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import pylupnt as pnt\n",
    "from pylupnt.numerics.frames import OCV_T_FLU, FLU_T_OCV\n",
    "from gluefactory.geometry.wrappers import Camera, Pose\n",
    "from gluefactory.eval.utils import eval_matches_epipolar\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Set up paths\n",
    "dataset_path = Path(\"/home/shared_ws6/data/feature_matching/unreal_training/\")\n",
    "scene_name = \"unreal\"\n",
    "scene_dir = dataset_path / scene_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, scene_name):\n",
    "    scene_dir = Path(dataset_path) / scene_name\n",
    "\n",
    "    with open(scene_dir / \"pairs.txt\") as f:\n",
    "        pairs = [tuple(line.strip().split()[:2]) for line in f if line.strip()]\n",
    "\n",
    "    with open(scene_dir / \"views.txt\") as f:\n",
    "        views = {parts[0]: parts[1:] for line in f if (parts := line.strip().split())}\n",
    "\n",
    "    return pairs, views, scene_dir\n",
    "\n",
    "\n",
    "pairs, views, scene_dir = load_dataset(dataset_path, scene_name)\n",
    "print(f\"Loaded {len(pairs)} pairs\\nFirst pair: {pairs[0]}\")\n",
    "\n",
    "if pairs:\n",
    "    frame_gaps = [\n",
    "        abs(int(a.replace(\".jpg\", \"\")) - int(b.replace(\".jpg\", \"\")))\n",
    "        for a, b in pairs[:100]\n",
    "    ]\n",
    "    print(\n",
    "        f\"Frame gaps (first 100 pairs): min={min(frame_gaps)}, max={max(frame_gaps)}, mean={np.mean(frame_gaps):.1f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_view(img_name, view_data, scene_dir):\n",
    "    R = np.array(view_data[0:9], dtype=np.float32).reshape(3, 3)\n",
    "    t = np.array(view_data[9:12], dtype=np.float32)\n",
    "    w, h = int(view_data[13]), int(view_data[14])\n",
    "    params = np.array(view_data[15:], dtype=np.float32)\n",
    "    img = cv2.cvtColor(\n",
    "        cv2.imread(str(scene_dir / \"images\" / img_name)), cv2.COLOR_BGR2RGB\n",
    "    )\n",
    "    img = img.astype(np.uint8)\n",
    "    with h5py.File(scene_dir / \"depths\" / img_name.replace(\".jpg\", \".h5\"), \"r\") as f:\n",
    "        depth = f[\"depth\"][:].astype(np.float32)\n",
    "    camera = Camera.from_colmap(\n",
    "        {\"model\": view_data[12], \"width\": w, \"height\": h, \"params\": params}\n",
    "    )\n",
    "    # Poses are in FLU frame, but gluefactory expects OCV frame\n",
    "    # If world_T_cam_ocv = world_T_cam_flu @ FLU_T_OCV (camera-to-world)\n",
    "    # Then T_w2cam_ocv = inv(world_T_cam_ocv) = OCV_T_FLU @ T_w2cam_flu (world-to-camera)\n",
    "    T_w2cam_flu = Pose.from_Rt(torch.from_numpy(R).float(), torch.from_numpy(t).float())\n",
    "    T_w2cam_ocv = Pose.from_4x4mat(torch.from_numpy(OCV_T_FLU).float()) @ T_w2cam_flu\n",
    "    return img, depth, camera, T_w2cam_ocv\n",
    "\n",
    "\n",
    "img0_name, img1_name = pairs[0]\n",
    "print(\n",
    "    f\"View {img0_name}: len={len(views[img0_name])}, first 20: {views[img0_name][:20]}\"\n",
    ")\n",
    "img0, depth0, camera0, T_w2cam0_gt = parse_view(img0_name, views[img0_name], scene_dir)\n",
    "img1, depth1, camera1, T_w2cam1_gt = parse_view(img1_name, views[img1_name], scene_dir)\n",
    "print(\n",
    "    f\"Image 0: {img0.shape}, depth: {depth0.shape}\\nImage 1: {img1.shape}, depth: {depth1.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction and matching (concise)\n",
    "extractor = pnt.FeatureExtractor.from_config({\"class\": \"SuperPoint\"})\n",
    "matcher_lightglue = pnt.FeatureMatcher.from_config(\n",
    "    {\"class\": \"LightGlue\", \"features\": \"superpoint\"}\n",
    ")\n",
    "matcher_superglue = pnt.FeatureMatcher.from_config({\"class\": \"SuperGlue\"})\n",
    "\n",
    "\n",
    "# Convert images to uint8 if needed\n",
    "def to_uint8(im):\n",
    "    if im.dtype in [np.float32, np.float64]:\n",
    "        return (im * 255).astype(np.uint8) if im.max() <= 1.0 else im.astype(np.uint8)\n",
    "    return im\n",
    "\n",
    "\n",
    "img0, img1 = to_uint8(img0), to_uint8(img1)\n",
    "\n",
    "for i, im in enumerate([img0, img1]):\n",
    "    print(\n",
    "        f\"Image {i} dtype: {im.dtype}, shape: {im.shape}, range: [{im.min()}, {im.max()}]\"\n",
    "    )\n",
    "\n",
    "feats0, feats1 = extractor.extract(img0), extractor.extract(img1)\n",
    "matches_superglue = matcher_superglue.match(feats0, feats1)\n",
    "matches_lightglue = matcher_lightglue.match(feats0, feats1)\n",
    "matches = matches_lightglue  # LightGlue by default\n",
    "\n",
    "print(f\"Features 0: {len(feats0)}, Features 1: {len(feats1)}\")\n",
    "print(\n",
    "    f\"Matches (SuperGlue): {len(matches_superglue)}, Matches (LightGlue): {len(matches_lightglue)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_keypoints(img0, img1, feats0, feats1, max_keypoints=1000):\n",
    "    \"\"\"Visualize all extracted keypoints using cyan and magenta.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    kp0 = np.asarray(feats0.uv.cpu() if hasattr(feats0.uv, \"cpu\") else feats0.uv)\n",
    "    kp1 = np.asarray(feats1.uv.cpu() if hasattr(feats1.uv, \"cpu\") else feats1.uv)\n",
    "    idx0 = (\n",
    "        np.random.choice(len(kp0), min(max_keypoints, len(kp0)), replace=False)\n",
    "        if len(kp0) > max_keypoints\n",
    "        else np.arange(len(kp0))\n",
    "    )\n",
    "    idx1 = (\n",
    "        np.random.choice(len(kp1), min(max_keypoints, len(kp1)), replace=False)\n",
    "        if len(kp1) > max_keypoints\n",
    "        else np.arange(len(kp1))\n",
    "    )\n",
    "\n",
    "    axes[0].imshow(img0)\n",
    "    axes[0].scatter(\n",
    "        kp0[idx0, 0], kp0[idx0, 1], c=\"#00FFFF\", s=10, alpha=0.6, label=\"cyan\"\n",
    "    )\n",
    "    axes[0].set_title(f\"Image 0: {len(kp0)} kpts (showing {len(idx0)})\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(img1)\n",
    "    axes[1].scatter(\n",
    "        kp1[idx1, 0], kp1[idx1, 1], c=\"#FF00FF\", s=10, alpha=0.6, label=\"magenta\"\n",
    "    )\n",
    "    axes[1].set_title(f\"Image 1: {len(kp1)} kpts (showing {len(idx1)})\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Image 0: {len(kp0)} keypoints; Image 1: {len(kp1)} keypoints\")\n",
    "    if hasattr(feats0, \"scores\") and hasattr(feats1, \"scores\"):\n",
    "        for i, (scores, tag) in enumerate(\n",
    "            [(feats0.scores, \"Image 0\"), (feats1.scores, \"Image 1\")]\n",
    "        ):\n",
    "            s = np.asarray(scores.cpu() if hasattr(scores, \"cpu\") else scores)\n",
    "            print(\n",
    "                f\"{tag} scores: min={s.min():.3f}, max={s.max():.3f}, mean={s.mean():.3f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def visualize_matches(img0, img1, feats0, feats1, matches, max_matches=200):\n",
    "    \"\"\"Visualize matches between images using cyan, magenta, and yellow.\"\"\"\n",
    "    h0, w0 = img0.shape[:2]\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    img_comb = np.zeros((max(h0, h1), w0 + w1, 3), dtype=img0.dtype)\n",
    "    img_comb[:h0, :w0] = img0\n",
    "    img_comb[:h1, w0:] = img1\n",
    "    kp0 = np.asarray(feats0.uv.cpu() if hasattr(feats0.uv, \"cpu\") else feats0.uv)\n",
    "    kp1 = np.asarray(feats1.uv.cpu() if hasattr(feats1.uv, \"cpu\") else feats1.uv)\n",
    "    ms = (\n",
    "        matches.indexes[:max_matches]\n",
    "        if max_matches is not None and len(matches.indexes) > 0\n",
    "        else matches.indexes\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    ax.imshow(img_comb)\n",
    "    for idx in ms:\n",
    "        pt0, pt1 = kp0[idx[0]], kp1[idx[1]] + np.array([w0, 0])\n",
    "        # match lines = yellow, keypoints = cyan (img0), magenta (img1)\n",
    "        ax.plot(\n",
    "            [pt0[0], pt1[0]],\n",
    "            [pt0[1], pt1[1]],\n",
    "            color=\"#FFFF00\",\n",
    "            alpha=0.3,\n",
    "            linewidth=0.5,\n",
    "        )  # yellow\n",
    "        ax.plot(pt0[0], pt0[1], \"o\", color=\"#00FFFF\", markersize=2)  # cyan\n",
    "        ax.plot(pt1[0], pt1[1], \"o\", color=\"#FF00FF\", markersize=2)  # magenta\n",
    "    ax.set_title(f\"Matches (showing {len(ms)} of {len(matches)})\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"=== All Keypoints ===\")\n",
    "visualize_keypoints(img0, img1, feats0, feats1)\n",
    "print(\"\\n=== Matches ===\")\n",
    "print(f\"Total matches found: {len(matches)}\")\n",
    "print(f\"Match ratio: {len(matches)/min(len(feats0),len(feats1))*100:.2f}%\")\n",
    "print(f\"Image pair: {img0_name} <-> {img1_name}\")\n",
    "\n",
    "idx0, idx1 = int(img0_name.replace(\".jpg\", \"\")), int(img1_name.replace(\".jpg\", \"\"))\n",
    "print(f\"Frame gap: {abs(idx1-idx0)} frames\")\n",
    "\n",
    "match_scores = matches.distances\n",
    "print(\n",
    "    f\"Match scores: min={match_scores.min():.3f}, max={match_scores.max():.3f}, mean={match_scores.mean():.3f}\"\n",
    ")\n",
    "visualize_matches(img0, img1, feats0, feats1, matches, max_matches=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched_keypoints(feats0, feats1, matches):\n",
    "    idx = matches.indexes\n",
    "    return feats0.uv[idx[:, 0]], feats1.uv[idx[:, 1]]\n",
    "\n",
    "\n",
    "def sample_depth_at_keypoints(depth_map, kpts):\n",
    "    k = np.round(kpts).astype(int)\n",
    "    k[:, 0] = np.clip(k[:, 0], 0, depth_map.shape[1] - 1)\n",
    "    k[:, 1] = np.clip(k[:, 1], 0, depth_map.shape[0] - 1)\n",
    "    d = depth_map[k[:, 1], k[:, 0]]\n",
    "    valid = (d > 0) & (d < 1000)\n",
    "    return d, valid\n",
    "\n",
    "\n",
    "matched_kp0, matched_kp1 = get_matched_keypoints(feats0, feats1, matches)\n",
    "depth0_vals, valid0 = sample_depth_at_keypoints(depth0, matched_kp0)\n",
    "\n",
    "print(f\"Total matches: {len(matched_kp0)}\")\n",
    "print(f\"Valid depth matches: {valid0.sum()}\")\n",
    "print(\n",
    "    f\"Depth range: {depth0_vals[valid0].min():.2f} - {depth0_vals[valid0].max():.2f} m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_3d_world(matched_kp0, depth0_vals, valid0, camera0, T_w2cam0_gt):\n",
    "    \"\"\"Convert 2D keypoints + depth to 3D points in world frame.\n",
    "\n",
    "    Note: T_w2cam0_gt is in OCV frame (for gluefactory), but we need to convert\n",
    "    points to FLU frame for PnP solver (lupnt uses FLU).\n",
    "    \"\"\"\n",
    "    intrinsics_dict = {\n",
    "        \"fx\": float(camera0._data[2]),\n",
    "        \"fy\": float(camera0._data[3]),\n",
    "        \"cx\": float(camera0._data[4]),\n",
    "        \"cy\": float(camera0._data[5]),\n",
    "    }\n",
    "\n",
    "    # Convert T_w2cam0_gt from OCV back to FLU\n",
    "    T_w2cam0_gt_flu = (\n",
    "        Pose.from_4x4mat(torch.from_numpy(FLU_T_OCV).float()) @ T_w2cam0_gt\n",
    "    )\n",
    "\n",
    "    # Compute 3D points in FLU camera frame, then transform to world frame\n",
    "    # PnP solver expects points in world frame with FLU camera convention\n",
    "    u, v = matched_kp0[valid0][:, 0], matched_kp0[valid0][:, 1]\n",
    "    fx, fy = intrinsics_dict[\"fx\"], intrinsics_dict[\"fy\"]\n",
    "    cx, cy = intrinsics_dict[\"cx\"], intrinsics_dict[\"cy\"]\n",
    "    x_cam_ocv = (u - cx) * depth0_vals[valid0] / fx\n",
    "    y_cam_ocv = (v - cy) * depth0_vals[valid0] / fy\n",
    "    z_cam_ocv = depth0_vals[valid0]\n",
    "    xyz_cam_ocv = np.stack([x_cam_ocv, y_cam_ocv, z_cam_ocv], axis=-1)\n",
    "\n",
    "    # Convert from OCV to FLU camera frame\n",
    "    xyz_cam_flu = pnt.apply_transform(FLU_T_OCV, xyz_cam_ocv)\n",
    "\n",
    "    # Transform to world frame using FLU world_T_cam\n",
    "    world_T_cam0_flu = pnt.make_transform(*T_w2cam0_gt_flu.inv().numpy())\n",
    "    pts_w_flu = pnt.apply_transform(world_T_cam0_flu, xyz_cam_flu)\n",
    "\n",
    "    return pts_w_flu, xyz_cam_flu\n",
    "\n",
    "\n",
    "xyz_w, xyz_cam_flu = convert_to_3d_world(\n",
    "    matched_kp0, depth0_vals, valid0, camera0, T_w2cam0_gt\n",
    ")\n",
    "\n",
    "print(f\"3D points in world (FLU): {xyz_w.shape}\")\n",
    "print(\n",
    "    f\"3D points range: x[{xyz_w[:, 0].min():.2f}, {xyz_w[:, 0].max():.2f}], \"\n",
    "    f\"y[{xyz_w[:, 1].min():.2f}, {xyz_w[:, 1].max():.2f}], \"\n",
    "    f\"z[{xyz_w[:, 2].min():.2f}, {xyz_w[:, 2].max():.2f}])\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug evaluation metrics - convert to gluefactory format and run eval functions\n",
    "from gluefactory.eval.utils import (\n",
    "    eval_matches_depth,\n",
    "    eval_matches_epipolar,\n",
    "    eval_relative_pose_robust,\n",
    ")\n",
    "from pylupnt.numerics import ensure_torch\n",
    "\n",
    "\n",
    "def convert_to_gluefactory_format(\n",
    "    img0, img1, depth0, depth1, camera0, camera1, T_w2cam0, T_w2cam1\n",
    "):\n",
    "    \"\"\"Convert images and poses to glue-factory format.\"\"\"\n",
    "    # Add batch dimension to cameras if needed\n",
    "    if camera0._data.ndim == 1:\n",
    "        camera0 = Camera(camera0._data.unsqueeze(0))\n",
    "    if camera1._data.ndim == 1:\n",
    "        camera1 = Camera(camera1._data.unsqueeze(0))\n",
    "\n",
    "    return {\n",
    "        \"view0\": {\n",
    "            \"image\": ensure_torch(img0, channels=3, device=\"cpu\", batch_dim=True),\n",
    "            \"camera\": camera0,\n",
    "            \"depth\": ensure_torch(depth0, device=\"cpu\", batch_dim=True),\n",
    "            \"T_w2cam\": T_w2cam0,\n",
    "        },\n",
    "        \"view1\": {\n",
    "            \"image\": ensure_torch(img1, channels=3, device=\"cpu\", batch_dim=True),\n",
    "            \"camera\": camera1,\n",
    "            \"depth\": ensure_torch(depth1, device=\"cpu\", batch_dim=True),\n",
    "            \"T_w2cam\": T_w2cam1,\n",
    "        },\n",
    "        \"T_0to1\": T_w2cam1 @ T_w2cam0.inv(),\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_features_to_gluefactory_format(feats1, feats2, matches):\n",
    "    \"\"\"Convert pylupnt Features and Matches to glue-factory format.\"\"\"\n",
    "    kpts0 = torch.from_numpy(feats1.uv.copy()).float()\n",
    "    kpts1 = torch.from_numpy(feats2.uv.copy()).float()\n",
    "    matches0 = torch.full((len(feats1),), -1, dtype=torch.long)\n",
    "    matching_scores0 = torch.zeros(len(feats1), dtype=torch.float32)\n",
    "\n",
    "    if len(matches.indexes) > 0:\n",
    "        idx0 = torch.from_numpy(matches.indexes[:, 0].copy()).long()\n",
    "        idx1 = torch.from_numpy(matches.indexes[:, 1].copy()).long()\n",
    "        matches0[idx0] = idx1\n",
    "        max_dist = matches.distances.max() if len(matches.distances) > 0 else 1.0\n",
    "        scores = 1.0 - (matches.distances / (max_dist + 1e-6))\n",
    "        matching_scores0[idx0] = torch.from_numpy(scores).float()\n",
    "\n",
    "    return {\n",
    "        \"keypoints0\": kpts0,\n",
    "        \"keypoints1\": kpts1,\n",
    "        \"matches0\": matches0,\n",
    "        \"matching_scores0\": matching_scores0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Convert to gluefactory format\n",
    "data = convert_to_gluefactory_format(\n",
    "    img0, img1, depth0, depth1, camera0, camera1, T_w2cam0_gt, T_w2cam1_gt\n",
    ")\n",
    "pred = convert_features_to_gluefactory_format(feats0, feats1, matches)\n",
    "\n",
    "print(\"Data keys:\", list(data.keys()))\n",
    "print(\"Data view0 keys:\", list(data[\"view0\"].keys()))\n",
    "print(\"Pred keys:\", list(pred.keys()))\n",
    "print(\n",
    "    f\"Keypoints0 shape: {pred['keypoints0'].shape}, dtype: {pred['keypoints0'].dtype}\"\n",
    ")\n",
    "print(\n",
    "    f\"Keypoints1 shape: {pred['keypoints1'].shape}, dtype: {pred['keypoints1'].dtype}\"\n",
    ")\n",
    "print(\n",
    "    f\"Matches0 shape: {pred['matches0'].shape}, valid matches: {(pred['matches0'] >= 0).sum().item()}\"\n",
    ")\n",
    "print(f\"T_0to1 shape: {data['T_0to1'].shape}\")\n",
    "print(f\"Camera0 shape: {data['view0']['camera']._data.shape}\")\n",
    "print(f\"Camera1 shape: {data['view1']['camera']._data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epipolar evaluation\n",
    "from gluefactory.geometry.epipolar import generalized_epi_dist\n",
    "\n",
    "epipolar_results = eval_matches_epipolar(data, pred)\n",
    "print(\"Epipolar Results:\")\n",
    "for k, v in epipolar_results.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Compute epipolar errors for matched points\n",
    "kp0, kp1 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
    "m0 = pred[\"matches0\"]\n",
    "valid_matches = m0 >= 0\n",
    "pts0, pts1 = kp0[valid_matches], kp1[m0[valid_matches]]\n",
    "\n",
    "n_epi_err = generalized_epi_dist(\n",
    "    pts0[None],\n",
    "    pts1[None],\n",
    "    data[\"view0\"][\"camera\"],\n",
    "    data[\"view1\"][\"camera\"],\n",
    "    data[\"T_0to1\"],\n",
    "    False,\n",
    "    essential=True,\n",
    ")[0]\n",
    "\n",
    "print(\n",
    "    f\"\\nEpipolar errors: mean={n_epi_err.mean().item():.6f}, median={n_epi_err.median().item():.6f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Matches below thresholds: <1e-4: {(n_epi_err < 1e-4).sum().item()}, <5e-4: {(n_epi_err < 5e-4).sum().item()}, <1e-3: {(n_epi_err < 1e-3).sum().item()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize epipolar errors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Histogram of epipolar errors\n",
    "axes[0].hist(n_epi_err.cpu().numpy(), bins=100, log=True, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(1e-4, color=\"r\", linestyle=\"--\", label=\"1e-4 threshold\")\n",
    "axes[0].axvline(5e-4, color=\"orange\", linestyle=\"--\", label=\"5e-4 threshold\")\n",
    "axes[0].axvline(1e-3, color=\"y\", linestyle=\"--\", label=\"1e-3 threshold\")\n",
    "axes[0].set_xlabel(\"Epipolar Error\")\n",
    "axes[0].set_ylabel(\"Count (log scale)\")\n",
    "axes[0].set_title(\"Distribution of Epipolar Errors\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# CDF of epipolar errors\n",
    "sorted_err = torch.sort(n_epi_err)[0].cpu().numpy()\n",
    "cumulative = np.arange(1, len(sorted_err) + 1) / len(sorted_err)\n",
    "axes[1].plot(sorted_err, cumulative, linewidth=2)\n",
    "axes[1].axvline(1e-4, color=\"r\", linestyle=\"--\", label=\"1e-4 threshold\")\n",
    "axes[1].axvline(5e-4, color=\"orange\", linestyle=\"--\", label=\"5e-4 threshold\")\n",
    "axes[1].axvline(1e-3, color=\"y\", linestyle=\"--\", label=\"1e-3 threshold\")\n",
    "axes[1].set_xlabel(\"Epipolar Error\")\n",
    "axes[1].set_ylabel(\"Cumulative Fraction\")\n",
    "axes[1].set_title(\"CDF of Epipolar Errors\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run depth evaluation and debug\n",
    "from gluefactory.geometry.depth import symmetric_reprojection_error\n",
    "\n",
    "depth_results = eval_matches_depth(data, pred)\n",
    "print(\"Depth/Reprojection Results:\")\n",
    "for k, v in depth_results.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Debug reprojection errors directly\n",
    "reproj_error, valid = symmetric_reprojection_error(\n",
    "    pts0[None],\n",
    "    pts1[None],\n",
    "    data[\"view0\"][\"camera\"],\n",
    "    data[\"view1\"][\"camera\"],\n",
    "    data[\"T_0to1\"],\n",
    "    data[\"view0\"][\"depth\"],\n",
    "    data[\"view1\"][\"depth\"],\n",
    ")\n",
    "reproj_error, valid = reproj_error[0], valid[0]\n",
    "\n",
    "print(f\"\\nReprojection errors:\")\n",
    "print(f\"  Shape: {reproj_error.shape}\")\n",
    "print(f\"  Valid matches: {valid.sum().item()} / {len(valid)}\")\n",
    "print(f\"  Valid error - Min: {reproj_error[valid].min().item():.3f}px\")\n",
    "print(f\"  Valid error - Max: {reproj_error[valid].max().item():.3f}px\")\n",
    "print(f\"  Valid error - Mean: {reproj_error[valid].mean().item():.3f}px\")\n",
    "print(f\"  Valid error - Median: {reproj_error[valid].median().item():.3f}px\")\n",
    "\n",
    "# Count matches below thresholds\n",
    "valid_errors = reproj_error[valid].nan_to_num(nan=float(\"inf\"))\n",
    "print(f\"\\nValid matches below thresholds:\")\n",
    "print(\n",
    "    f\"  < 1px: {(valid_errors < 1).sum().item()} ({(valid_errors < 1).float().mean().item()*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  < 3px: {(valid_errors < 3).sum().item()} ({(valid_errors < 3).float().mean().item()*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  < 5px: {(valid_errors < 5).sum().item()} ({(valid_errors < 5).float().mean().item()*100:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reprojection errors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "valid_errors = reproj_error[valid].nan_to_num(nan=float(\"inf\")).cpu().numpy()\n",
    "\n",
    "# Histogram of reprojection errors\n",
    "axes[0].hist(valid_errors, bins=100, log=True, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(1, color=\"r\", linestyle=\"--\", label=\"1px threshold\")\n",
    "axes[0].axvline(3, color=\"orange\", linestyle=\"--\", label=\"3px threshold\")\n",
    "axes[0].axvline(5, color=\"y\", linestyle=\"--\", label=\"5px threshold\")\n",
    "axes[0].set_xlabel(\"Reprojection Error (px)\")\n",
    "axes[0].set_ylabel(\"Count (log scale)\")\n",
    "axes[0].set_title(\"Distribution of Reprojection Errors (valid matches)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# CDF of reprojection errors\n",
    "sorted_err = np.sort(valid_errors)\n",
    "cumulative = np.arange(1, len(sorted_err) + 1) / len(sorted_err)\n",
    "axes[1].plot(sorted_err, cumulative, linewidth=2)\n",
    "axes[1].axvline(1, color=\"r\", linestyle=\"--\", label=\"1px threshold\")\n",
    "axes[1].axvline(3, color=\"orange\", linestyle=\"--\", label=\"3px threshold\")\n",
    "axes[1].axvline(5, color=\"y\", linestyle=\"--\", label=\"5px threshold\")\n",
    "axes[1].set_xlabel(\"Reprojection Error (px)\")\n",
    "axes[1].set_ylabel(\"Cumulative Fraction\")\n",
    "axes[1].set_title(\"CDF of Reprojection Errors\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run relative pose evaluation and debug\n",
    "eval_conf = {\"estimator\": \"opencv\", \"ransac_th\": 1.0}\n",
    "\n",
    "pose_results = eval_relative_pose_robust(data, pred, eval_conf)\n",
    "print(\"Relative Pose Results:\")\n",
    "for k, v in pose_results.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Check T_0to1 transform\n",
    "T_0to1 = data[\"T_0to1\"]\n",
    "print(f\"\\nT_0to1 transform:\")\n",
    "print(f\"  Shape: {T_0to1.shape}\")\n",
    "print(f\"  Translation norm: {torch.norm(T_0to1.t).item():.3f}\")\n",
    "print(f\"  Rotation matrix:\\n{T_0to1.R.numpy()}\")\n",
    "\n",
    "# Check camera calibration matrices\n",
    "K0 = data[\"view0\"][\"camera\"].calibration_matrix()\n",
    "K1 = data[\"view1\"][\"camera\"].calibration_matrix()\n",
    "print(f\"\\nCamera 0 K:\\n{K0.squeeze().numpy()}\")\n",
    "print(f\"\\nCamera 1 K:\\n{K1.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matches colored by epipolar error\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "h0, w0 = img0.shape[:2]\n",
    "h1, w1 = img1.shape[:2]\n",
    "img_comb = np.zeros((max(h0, h1), w0 + w1, 3), dtype=img0.dtype)\n",
    "img_comb[:h0, :w0] = img0\n",
    "img_comb[:h1, w0:] = img1\n",
    "\n",
    "ax.imshow(img_comb)\n",
    "\n",
    "# Color matches by epipolar error\n",
    "epi_err_np = n_epi_err.cpu().numpy()\n",
    "norm = plt.Normalize(vmin=epi_err_np.min(), vmax=epi_err_np.max())\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "for i, (pt0, pt1, err) in enumerate(zip(pts0, pts1, epi_err_np)):\n",
    "    if i % 10 == 0:  # Sample every 10th match for visualization\n",
    "        pt0_np = pt0.cpu().numpy() if torch.is_tensor(pt0) else pt0\n",
    "        pt1_np = pt1.cpu().numpy() if torch.is_tensor(pt1) else pt1\n",
    "        pt1_np[0] += w0\n",
    "\n",
    "        color = cmap(norm(err))\n",
    "        ax.plot(\n",
    "            [pt0_np[0], pt1_np[0]],\n",
    "            [pt0_np[1], pt1_np[1]],\n",
    "            color=color,\n",
    "            alpha=0.5,\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "        ax.plot(pt0_np[0], pt0_np[1], \"o\", color=color, markersize=2)\n",
    "        ax.plot(pt1_np[0], pt1_np[1], \"o\", color=color, markersize=2)\n",
    "\n",
    "ax.set_title(\n",
    "    f\"Matches colored by Epipolar Error (showing every 10th, total: {len(pts0)})\"\n",
    ")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Epipolar Error\", rotation=270, labelpad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug coordinate frames and transforms\n",
    "print(\"=== Coordinate Frame Debug ===\")\n",
    "print(f\"\\nT_w2cam0_gt (world to camera 0):\")\n",
    "print(f\"  Translation: {T_w2cam0_gt.t.numpy()}\")\n",
    "print(f\"  Rotation (first row): {T_w2cam0_gt.R[0].numpy()}\")\n",
    "\n",
    "print(f\"\\nT_w2cam1_gt (world to camera 1):\")\n",
    "print(f\"  Translation: {T_w2cam1_gt.t.numpy()}\")\n",
    "print(f\"  Rotation (first row): {T_w2cam1_gt.R[0].numpy()}\")\n",
    "\n",
    "# Compute T_0to1 manually\n",
    "T_0to1_manual = T_w2cam1_gt @ T_w2cam0_gt.inv()\n",
    "print(f\"\\nT_0to1_manual (camera 0 to camera 1):\")\n",
    "print(f\"  Translation: {T_0to1_manual.t.numpy()}\")\n",
    "print(f\"  Translation norm: {torch.norm(T_0to1_manual.t).item():.3f}\")\n",
    "print(f\"  Rotation (first row): {T_0to1_manual.R[0].numpy()}\")\n",
    "\n",
    "# Check what gluefactory computed\n",
    "T_0to1_gluefactory = data[\"T_0to1\"]\n",
    "print(f\"\\nT_0to1_gluefactory (from data dict):\")\n",
    "print(f\"  Translation: {T_0to1_gluefactory.t.numpy()}\")\n",
    "print(f\"  Translation norm: {torch.norm(T_0to1_gluefactory.t).item():.3f}\")\n",
    "print(f\"  Rotation (first row): {T_0to1_gluefactory.R[0].numpy()}\")\n",
    "\n",
    "# Check if they match\n",
    "print(\n",
    "    f\"\\nTransforms match: {torch.allclose(T_0to1_manual.R, T_0to1_gluefactory.R, atol=1e-5) and torch.allclose(T_0to1_manual.t, T_0to1_gluefactory.t, atol=1e-5)}\"\n",
    ")\n",
    "\n",
    "# Check camera coordinate frames\n",
    "print(f\"\\n=== Camera Info ===\")\n",
    "print(f\"Camera 0 size: {camera0.size.numpy()}\")\n",
    "print(f\"Camera 1 size: {camera1.size.numpy()}\")\n",
    "print(f\"Camera 0 K:\\n{camera0.calibration_matrix().squeeze().numpy()}\")\n",
    "print(f\"Camera 1 K:\\n{camera1.calibration_matrix().squeeze().numpy()}\")\n",
    "\n",
    "# Check a few matched points\n",
    "print(f\"\\n=== Sample Matched Points ===\")\n",
    "print(f\"First 5 matched points in image 0:\\n{pts0[:5].cpu().numpy()}\")\n",
    "print(f\"First 5 matched points in image 1:\\n{pts1[:5].cpu().numpy()}\")\n",
    "print(f\"Image 0 shape: {img0.shape}\")\n",
    "print(f\"Image 1 shape: {img1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the relative pose estimation - check what RANSAC is estimating\n",
    "from gluefactory.robust_estimators import load_estimator\n",
    "from gluefactory.geometry.epipolar import relative_pose_error\n",
    "\n",
    "estimator = load_estimator(\"relative_pose\", \"opencv\")(\n",
    "    {\"ransac_th\": 1.0, \"confidence\": 0.99}\n",
    ")\n",
    "data_est = {\n",
    "    \"m_kpts0\": pts0,\n",
    "    \"m_kpts1\": pts1,\n",
    "    \"camera0\": data[\"view0\"][\"camera\"][0],\n",
    "    \"camera1\": data[\"view1\"][\"camera\"][0],\n",
    "}\n",
    "est = estimator(data_est)\n",
    "\n",
    "print(\"=== RANSAC Pose Estimation Debug ===\")\n",
    "print(f\"Success: {est['success']}\")\n",
    "if est[\"success\"]:\n",
    "    M_est = est[\"M_0to1\"]\n",
    "    print(f\"\\nEstimated pose M_0to1:\")\n",
    "    print(f\"  Translation: {M_est.t.numpy()}\")\n",
    "    print(f\"  Translation norm: {torch.norm(M_est.t).item():.3f}\")\n",
    "    print(f\"  Rotation (first row): {M_est.R[0].numpy()}\")\n",
    "\n",
    "    print(f\"\\nGround truth T_0to1:\")\n",
    "    T_gt = data[\"T_0to1\"]\n",
    "    print(f\"  Translation: {T_gt.t.numpy()}\")\n",
    "    print(f\"  Translation norm: {torch.norm(T_gt.t).item():.3f}\")\n",
    "    print(f\"  Rotation (first row): {T_gt.R[0].numpy()}\")\n",
    "\n",
    "    # Compute errors\n",
    "    t_err, r_err = relative_pose_error(T_gt, M_est.R, M_est.t)\n",
    "    print(f\"\\nPose errors:\")\n",
    "    print(f\"  Translation error: {t_err.item():.3f} deg\")\n",
    "    print(f\"  Rotation error: {r_err.item():.3f} deg\")\n",
    "    print(f\"  Max error (rel_pose_error): {max(t_err.item(), r_err.item()):.3f} deg\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nInliers: {est['inliers'].sum().item()} / {len(est['inliers'])} ({est['inliers'].float().mean().item()*100:.1f}%)\"\n",
    "    )\n",
    "else:\n",
    "    print(\"RANSAC failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coordinate frame - the poses might be in OCV but gluefactory expects a different convention\n",
    "print(\"=== Coordinate Frame Check ===\")\n",
    "print(f\"\\nT_w2cam0_gt translation: {T_w2cam0_gt.t.numpy()}\")\n",
    "print(f\"T_w2cam1_gt translation: {T_w2cam1_gt.t.numpy()}\")\n",
    "\n",
    "# Check if poses are in OCV frame (Z forward, Y down, X right)\n",
    "# vs FLU frame (X forward, Y left, Z up)\n",
    "print(f\"\\nCamera 0 position in world (OCV): {T_w2cam0_gt.inv().t.numpy()}\")\n",
    "print(f\"Camera 1 position in world (OCV): {T_w2cam1_gt.inv().t.numpy()}\")\n",
    "\n",
    "# The issue: GT translation is mostly in -X, but estimated is mostly in -Z\n",
    "# This suggests the poses might need coordinate frame conversion\n",
    "print(f\"\\n=== Testing Coordinate Frame Conversion ===\")\n",
    "print(\"If poses are in OCV frame, we might need to convert to FLU or another frame\")\n",
    "print(\"OCV: Z forward, Y down, X right\")\n",
    "print(\"FLU: X forward, Y left, Z up\")\n",
    "\n",
    "# Check the relative translation direction\n",
    "rel_trans = T_w2cam1_gt.inv().t - T_w2cam0_gt.inv().t\n",
    "print(f\"\\nRelative translation in world frame: {rel_trans.numpy()}\")\n",
    "print(f\"Direction (normalized): {rel_trans / np.linalg.norm(rel_trans)}\")\n",
    "\n",
    "# Compare with T_0to1 translation\n",
    "print(f\"T_0to1 translation (camera 0 to 1): {T_0to1.t.numpy()}\")\n",
    "print(\n",
    "    f\"T_0to1 translation direction (normalized): {(T_0to1.t / torch.norm(T_0to1.t)).numpy()}\"\n",
    ")\n",
    "\n",
    "# The estimated translation from RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concise 3D visualization using Plotly\n",
    "def visualize_3d_scene(xyz_w, T_w2cam0_gt, T_w2cam1_gt, T_w2cam1_est=None):\n",
    "    \"\"\"Visualize 3D scene with camera poses. Converts poses to FLU for consistent visualization.\"\"\"\n",
    "    n_points = min(1000, len(xyz_w))\n",
    "    points_sample = xyz_w[np.random.choice(len(xyz_w), n_points, replace=False)]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=points_sample[:, 0],\n",
    "            y=points_sample[:, 1],\n",
    "            z=points_sample[:, 2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=2, color=\"blue\", opacity=0.3),\n",
    "            name=\"3D Points\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert poses to FLU for visualization\n",
    "    for T_ocv, color, name in [\n",
    "        (T_w2cam0_gt, \"green\", \"Camera 0 (GT)\"),\n",
    "        (T_w2cam1_gt, \"red\", \"Camera 1 (GT)\"),\n",
    "        (T_w2cam1_est, \"orange\", \"Camera 1 (Est)\"),\n",
    "    ]:\n",
    "        if T_ocv is None:\n",
    "            continue\n",
    "        # Convert from OCV to FLU\n",
    "        T_flu = Pose.from_4x4mat(torch.from_numpy(FLU_T_OCV).float()) @ T_ocv\n",
    "        p = T_flu.inv().t.numpy()\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[p[0]],\n",
    "                y=[p[1]],\n",
    "                z=[p[2]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=10, color=color, symbol=\"diamond\"),\n",
    "                name=name,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"3D Scene with Camera Poses (FLU)\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"X (m)\",\n",
    "            yaxis_title=\"Y (m)\",\n",
    "            zaxis_title=\"Z (m)\",\n",
    "            aspectmode=\"data\",\n",
    "        ),\n",
    "        width=600,\n",
    "        height=400,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "visualize_3d_scene(xyz_w, T_w2cam0_gt, T_w2cam1_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pnp_absolute_localization(xyz_w, matched_kp1, valid0, camera1, T_w2cam_prev):\n",
    "    \"\"\"\n",
    "    Estimate camera 1 pose using PnP with previous camera pose as initial guess.\n",
    "\n",
    "    Args:\n",
    "        xyz_w: 3D points in world frame [N, 3] (FLU frame for lupnt PnP)\n",
    "        matched_kp1: Matched keypoints in image 1 [N, 2]\n",
    "        valid0: Valid depth mask [N]\n",
    "        camera1: Camera 1 object\n",
    "        T_w2cam_prev: Previous camera pose (T_w2cam0) in OCV frame, will be converted to FLU\n",
    "    \"\"\"\n",
    "    K1 = camera1.calibration_matrix().numpy()\n",
    "\n",
    "    # Convert pose from OCV to FLU for PnP solver (lupnt uses FLU)\n",
    "    T_w2cam_prev_flu = (\n",
    "        Pose.from_4x4mat(torch.from_numpy(FLU_T_OCV).float()) @ T_w2cam_prev\n",
    "    )\n",
    "    tgt_T_src_guess = pnt.make_transform(*T_w2cam_prev_flu.numpy())\n",
    "\n",
    "    pnp_solver = pnt.PnpSolver(\n",
    "        {\"threshold\": 1.0, \"confidence\": 0.99, \"max_iterations\": 10000}\n",
    "    )\n",
    "    pnp_result = pnp_solver.solve(xyz_w, matched_kp1[valid0], K1, tgt_T_src_guess)\n",
    "\n",
    "    if not pnp_result.success:\n",
    "        return None, None, None\n",
    "\n",
    "    # PnP result is in FLU, convert back to OCV for comparison with GT (which is in OCV)\n",
    "    # Ensure both are float32 to avoid dtype mismatch\n",
    "    T_w2cam1_est_flu = pnp_result.tgt_T_src.astype(np.float32)\n",
    "    T_w2cam1_est_ocv = Pose.from_4x4mat(\n",
    "        torch.from_numpy(OCV_T_FLU).float()\n",
    "    ) @ Pose.from_4x4mat(torch.from_numpy(T_w2cam1_est_flu).float())\n",
    "    inliers = np.array(pnp_result.inliers)\n",
    "    return T_w2cam1_est_ocv, inliers, xyz_w\n",
    "\n",
    "\n",
    "# Use previous camera pose (T_w2cam0_gt) as initial guess for camera 1 pose estimation\n",
    "# Note: T_w2cam0_gt is in OCV frame (for gluefactory), xyz_w is in FLU frame (for PnP)\n",
    "T_w2cam1_est, inliers, points_3d_world_flu = run_pnp_absolute_localization(\n",
    "    xyz_w, matched_kp1, valid0, camera1, T_w2cam0_gt\n",
    ")\n",
    "\n",
    "if T_w2cam1_est is not None:\n",
    "    print(f\"PnP succeeded with {len(inliers)} inliers\")\n",
    "\n",
    "    # Compute errors (both poses are now in OCV frame)\n",
    "    # Translation error: use camera position in world frame (not T_w2cam translation component)\n",
    "    cam_pos_est = T_w2cam1_est.inv().t.numpy()\n",
    "    cam_pos_gt = T_w2cam1_gt.inv().t.numpy()\n",
    "    t_error = np.linalg.norm(cam_pos_est - cam_pos_gt)\n",
    "    R_est = T_w2cam1_est.R\n",
    "    R_diff = R_est.T @ T_w2cam1_gt.R\n",
    "    angle = np.clip((np.trace(R_diff) - 1) / 2, -1, 1)\n",
    "    r_error = np.rad2deg(np.arccos(angle))\n",
    "\n",
    "    print(f\"GT camera 1 position: {cam_pos_gt}\")\n",
    "    print(f\"Est camera 1 position: {cam_pos_est}\")\n",
    "    print(f\"Translation error: {t_error:.3f} m\")\n",
    "    print(f\"Rotation error: {r_error:.3f} deg\")\n",
    "\n",
    "    # Visualize (poses will be converted to FLU inside the function)\n",
    "    visualize_3d_scene(xyz_w, T_w2cam0_gt, T_w2cam1_gt, T_w2cam1_est)\n",
    "else:\n",
    "    print(\"PnP failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reprojection errors\n",
    "def visualize_reprojection_errors(\n",
    "    img1,\n",
    "    points_3d_world_flu,\n",
    "    matched_kp1_valid,\n",
    "    T_w2cam1_gt,\n",
    "    T_w2cam1_est,\n",
    "    camera1,\n",
    "    inliers,\n",
    "):\n",
    "    K1_t = camera1.calibration_matrix()\n",
    "    if isinstance(K1_t, torch.Tensor):\n",
    "        K1 = K1_t[0].cpu().numpy() if K1_t.ndim > 2 else K1_t.cpu().numpy()\n",
    "    else:\n",
    "        K1 = K1_t[0] if K1_t.ndim > 2 else K1_t\n",
    "    K1 = np.array(K1, dtype=np.float64)\n",
    "\n",
    "    # Project 3D points using GT and estimated poses\n",
    "\n",
    "    def project_points(points_3d, T_w2cam, K):\n",
    "        # Transform to camera frame\n",
    "        R = T_w2cam.R.numpy()\n",
    "        t = T_w2cam.t.numpy()\n",
    "        points_cam = (points_3d @ R.T) + t\n",
    "\n",
    "        # Project\n",
    "        fx, fy, cx, cy = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n",
    "        x = points_cam[:, 0] / points_cam[:, 2]\n",
    "        y = points_cam[:, 1] / points_cam[:, 2]\n",
    "        u = fx * x + cx\n",
    "        v = fy * y + cy\n",
    "\n",
    "        return np.stack([u, v], axis=1)\n",
    "\n",
    "    proj_gt = project_points(points_3d_world_flu, T_w2cam1_gt, K1)\n",
    "    proj_est = project_points(points_3d_world_flu, T_w2cam1_est, K1)\n",
    "\n",
    "    # Compute reprojection errors\n",
    "    errors_gt = np.linalg.norm(proj_gt - matched_kp1_valid, axis=1)\n",
    "    errors_est = np.linalg.norm(proj_est - matched_kp1_valid, axis=1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    # GT reprojections\n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].scatter(\n",
    "        matched_kp1_valid[:, 0],\n",
    "        matched_kp1_valid[:, 1],\n",
    "        c=\"green\",\n",
    "        s=10,\n",
    "        alpha=0.5,\n",
    "        label=\"Detected\",\n",
    "    )\n",
    "    axes[0].scatter(\n",
    "        proj_gt[:, 0], proj_gt[:, 1], c=\"red\", s=10, alpha=0.5, label=\"GT Projected\"\n",
    "    )\n",
    "    for i in range(min(100, len(matched_kp1_valid))):\n",
    "        axes[0].plot(\n",
    "            [matched_kp1_valid[i, 0], proj_gt[i, 0]],\n",
    "            [matched_kp1_valid[i, 1], proj_gt[i, 1]],\n",
    "            \"r-\",\n",
    "            alpha=0.3,\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "    axes[0].set_title(f\"GT Reprojections (mean error: {errors_gt.mean():.2f} px)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Estimated reprojections\n",
    "    axes[1].imshow(img1)\n",
    "    axes[1].scatter(\n",
    "        matched_kp1_valid[:, 0],\n",
    "        matched_kp1_valid[:, 1],\n",
    "        c=\"green\",\n",
    "        s=10,\n",
    "        alpha=0.5,\n",
    "        label=\"Detected\",\n",
    "    )\n",
    "    axes[1].scatter(\n",
    "        proj_est[:, 0], proj_est[:, 1], c=\"blue\", s=10, alpha=0.5, label=\"Est Projected\"\n",
    "    )\n",
    "    inlier_mask = np.zeros(len(matched_kp1_valid), dtype=bool)\n",
    "    inlier_mask[inliers.flatten()] = True\n",
    "    for i in range(min(100, len(matched_kp1_valid))):\n",
    "        color = \"g-\" if inlier_mask[i] else \"r-\"\n",
    "        axes[1].plot(\n",
    "            [matched_kp1_valid[i, 0], proj_est[i, 0]],\n",
    "            [matched_kp1_valid[i, 1], proj_est[i, 1]],\n",
    "            color,\n",
    "            alpha=0.3,\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "    axes[1].set_title(f\"Est Reprojections (mean error: {errors_est.mean():.2f} px)\")\n",
    "    axes[1].legend()\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Error histogram\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.hist(errors_gt, bins=50, alpha=0.5, label=\"GT\", color=\"red\")\n",
    "    ax.hist(errors_est, bins=50, alpha=0.5, label=\"Est\", color=\"blue\")\n",
    "    ax.set_xlabel(\"Reprojection Error (px)\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(\"Reprojection Error Distribution\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if T_w2cam1_est is not None:\n",
    "    visualize_reprojection_errors(\n",
    "        img1,\n",
    "        xyz_w,\n",
    "        matched_kp1[valid0],\n",
    "        T_w2cam1_gt,\n",
    "        T_w2cam1_est,\n",
    "        camera1,\n",
    "        inliers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lupnt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
