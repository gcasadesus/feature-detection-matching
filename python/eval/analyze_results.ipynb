{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Increase plot resolution\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Directories\n",
    "models_dir = Path(\n",
    "    \"/home/guillemc/dev/LuPNT-private/output/2025_FeatureMatching/eval_results_models\"\n",
    ")\n",
    "legacy_dir = Path(\n",
    "    \"/home/guillemc/dev/LuPNT-private/output/2025_FeatureMatching/eval_results\"\n",
    ")\n",
    "\n",
    "all_data = []\n",
    "all_per_pair_list = []\n",
    "\n",
    "# Dataset Name Mapping (Prettify)\n",
    "dataset_map = {\n",
    "    \"short_base\": \"Baseline\",\n",
    "    \"short_camera_effects\": \"Camera Effects\",\n",
    "    \"short_higher_elevation\": \"Higher Sun Elevation\",\n",
    "    \"short_no_lights\": \"No Lights\",\n",
    "    \"long_base\": \"Long Base\",\n",
    "    \"long_camera_effects\": \"Long Camera Effects\",\n",
    "    \"long_higher_elevation\": \"Long Higher Elevation\",\n",
    "    \"long_no_lights\": \"Long No Lights\",\n",
    "    \"rover_0\": \"Spirals\",\n",
    "}\n",
    "\n",
    "# Model Name Mapping (Renaming)\n",
    "model_name_map = {\n",
    "    \"SuperPoint+LightGlue_Spirals\": \"Finetuned\",\n",
    "    # We will filter out the others, but mapping them just in case\n",
    "    \"lightglue_spirals_v1\": \"SuperPoint+LightGlue (Spirals)\",\n",
    "    \"lightglue_unreal_base1\": \"SuperPoint+LightGlue (Traverse)\",\n",
    "    \"spirals_seg_rover0\": \"SuperPoint+LightGlue (Spirals+Segmentation)\",\n",
    "}\n",
    "\n",
    "# Skip list (Dumb/Test models AND models user wants to hide)\n",
    "skip_patterns = [\n",
    "    \"semantic_test\",\n",
    "    \"local_traverse_fov90\",\n",
    "    \"spirals_20251211\",\n",
    "    # User requested ONLY \"Spirals Legacy\" (now \"Finetuned\")\n",
    "    \"lightglue_spirals_v1\",\n",
    "    \"lightglue_unreal_base1\",\n",
    "    \"spirals_seg_rover0\",\n",
    "]\n",
    "\n",
    "# --- 1. Load Trained Models ---\n",
    "print(f\"Scanning models in {models_dir}...\")\n",
    "if models_dir.exists():\n",
    "    for model_path in models_dir.iterdir():\n",
    "        if not model_path.is_dir():\n",
    "            continue\n",
    "        raw_model_name = model_path.name\n",
    "\n",
    "        # Filtering Models\n",
    "        if any(skip in raw_model_name for skip in skip_patterns):\n",
    "            continue\n",
    "\n",
    "        # Determine Display Name\n",
    "        model_display_name = raw_model_name  # Default\n",
    "\n",
    "        # Check for user-defined mapping (prefix match)\n",
    "        for key, val in model_name_map.items():\n",
    "            if key in raw_model_name:\n",
    "                model_display_name = val\n",
    "                break\n",
    "\n",
    "        # Fallback for others\n",
    "        if model_display_name == raw_model_name and \"spirals\" in raw_model_name.lower():\n",
    "            model_display_name = f\"Finetuned ({raw_model_name})\"\n",
    "\n",
    "        for dataset_path in model_path.iterdir():\n",
    "            if not dataset_path.is_dir():\n",
    "                continue\n",
    "            raw_dataset_name = dataset_path.name\n",
    "\n",
    "            # Filter Datasets: ONLY short_*, exclude rover_0/Spirals\n",
    "            if \"short\" not in raw_dataset_name:\n",
    "                continue\n",
    "\n",
    "            dataset_name = dataset_map.get(\n",
    "                raw_dataset_name, raw_dataset_name.replace(\"_\", \" \").title()\n",
    "            )\n",
    "\n",
    "            for step_file in dataset_path.glob(\"step_*.pkl\"):\n",
    "                try:\n",
    "                    step = int(step_file.stem.split(\"_\")[1])\n",
    "                    with open(step_file, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "\n",
    "                    metrics = data[\"results\"]\n",
    "                    for method, res in metrics.items():\n",
    "                        # Summary\n",
    "                        if \"summary\" in res:\n",
    "                            row = res[\"summary\"].copy()\n",
    "                            row[\"Model Type\"] = \"Finetuned\"\n",
    "                            row[\"Model\"] = model_display_name\n",
    "                            row[\"Method\"] = method\n",
    "                            row[\"Dataset\"] = dataset_name\n",
    "                            row[\"Step\"] = step\n",
    "                            if \"abs_loc_t_error\" in row:\n",
    "                                row[\"Abs Trans Error (m)\"] = row[\"abs_loc_t_error\"]\n",
    "                            if \"rel_pose_r_error\" in row:\n",
    "                                row[\"Rel Rot Error (deg)\"] = row[\"rel_pose_r_error\"]\n",
    "                            all_data.append(row)\n",
    "\n",
    "                        # Per Pair\n",
    "                        if \"per_pair\" in res:\n",
    "                            pp_data = res[\"per_pair\"]\n",
    "                            if \"num_matches\" in pp_data and isinstance(\n",
    "                                pp_data[\"num_matches\"], list\n",
    "                            ):\n",
    "                                target_len = len(pp_data[\"num_matches\"])\n",
    "                                filtered_pp = {\n",
    "                                    k: v\n",
    "                                    for k, v in pp_data.items()\n",
    "                                    if isinstance(v, list) and len(v) == target_len\n",
    "                                }\n",
    "                                try:\n",
    "                                    pp_df = pd.DataFrame(filtered_pp)\n",
    "                                    pp_df[\"Model Type\"] = \"Finetuned\"\n",
    "                                    pp_df[\"Model\"] = model_display_name\n",
    "                                    pp_df[\"Method\"] = method\n",
    "                                    pp_df[\"Dataset\"] = dataset_name\n",
    "                                    pp_df[\"Step\"] = step\n",
    "                                    if \"Frame\" not in pp_df.columns:\n",
    "                                        pp_df[\"Frame\"] = pp_df.index * step\n",
    "                                    all_per_pair_list.append(pp_df)\n",
    "                                except:\n",
    "                                    pass\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# --- 2. Load Legacy Results (Baselines) ---\n",
    "print(f\"Scanning legacy results in {legacy_dir}...\")\n",
    "if legacy_dir.exists():\n",
    "    for dataset_path in legacy_dir.iterdir():\n",
    "        if not dataset_path.is_dir():\n",
    "            continue\n",
    "        raw_dataset_name = dataset_path.name\n",
    "        dataset_name = dataset_map.get(\n",
    "            raw_dataset_name, raw_dataset_name.replace(\"_\", \" \").title()\n",
    "        )\n",
    "\n",
    "        for agent_path in dataset_path.iterdir():  # e.g. rover_0\n",
    "            if not agent_path.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Filter Datasets: ONLY short_*, exclude rover_0/Spirals\n",
    "            if \"short\" not in raw_dataset_name:\n",
    "                continue\n",
    "\n",
    "            for step_file in agent_path.glob(\"*.pkl\"):\n",
    "                try:\n",
    "                    parts = step_file.stem.split(\"_\")\n",
    "                    if len(parts) >= 2 and parts[0] == \"step\":\n",
    "                        try:\n",
    "                            step = int(parts[1])\n",
    "                        except:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    with open(step_file, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "\n",
    "                    metrics = data.get(\"results\", {})\n",
    "                    for method, res in metrics.items():\n",
    "                        # Determine Label: Just \"Detection+Matching\" (e.g. SuperPoint+LightGlue)\n",
    "                        # Remove \"Baseline\" prefix to match user request\n",
    "                        model_label = method\n",
    "\n",
    "                        # Summary\n",
    "                        if \"summary\" in res:\n",
    "                            row = res[\"summary\"].copy()\n",
    "                            row[\"Model Type\"] = \"Baseline\"\n",
    "                            row[\"Model\"] = (\n",
    "                                model_label  # Use the method name directly as the Model label\n",
    "                            )\n",
    "                            row[\"Method\"] = method\n",
    "                            row[\"Dataset\"] = dataset_name\n",
    "                            row[\"Step\"] = step\n",
    "                            if \"rel_pose_r_error\" in row:\n",
    "                                row[\"Rel Rot Error (deg)\"] = row[\"rel_pose_r_error\"]\n",
    "                            if \"rel_pose_t_error\" in row:\n",
    "                                row[\"Trans Error (m)\"] = row[\"rel_pose_t_error\"]\n",
    "                            all_data.append(row)\n",
    "\n",
    "                        # Per Pair\n",
    "                        if \"per_pair\" in res:\n",
    "                            pp_data = res[\"per_pair\"]\n",
    "                            if \"num_matches\" in pp_data and isinstance(\n",
    "                                pp_data[\"num_matches\"], list\n",
    "                            ):\n",
    "                                target_len = len(pp_data[\"num_matches\"])\n",
    "                                filtered_pp = {\n",
    "                                    k: v\n",
    "                                    for k, v in pp_data.items()\n",
    "                                    if isinstance(v, list) and len(v) == target_len\n",
    "                                }\n",
    "                                try:\n",
    "                                    pp_df = pd.DataFrame(filtered_pp)\n",
    "                                    pp_df[\"Model Type\"] = \"Baseline\"\n",
    "                                    pp_df[\"Model\"] = model_label\n",
    "                                    pp_df[\"Method\"] = method\n",
    "                                    pp_df[\"Dataset\"] = dataset_name\n",
    "                                    pp_df[\"Step\"] = step\n",
    "                                    if \"Frame\" not in pp_df.columns:\n",
    "                                        pp_df[\"Frame\"] = pp_df.index * step\n",
    "                                    all_per_pair_list.append(pp_df)\n",
    "                                except:\n",
    "                                    pass\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "df_summary = pd.DataFrame(all_data)\n",
    "if all_per_pair_list:\n",
    "    df_per_pair = pd.concat(all_per_pair_list, ignore_index=True)\n",
    "    # UNIFIED LABEL COLUMN\n",
    "    # Just use 'Model' for the label. It now contains the pretty name or the method name.\n",
    "    df_per_pair[\"Labels\"] = df_per_pair[\"Model\"]\n",
    "else:\n",
    "    df_per_pair = pd.DataFrame()\n",
    "\n",
    "print(f\"Loaded {len(df_summary)} total summary records.\")\n",
    "if not df_summary.empty:\n",
    "    # Also for summary df\n",
    "    df_summary[\"Labels\"] = df_summary[\"Model\"]\n",
    "\n",
    "    # 2b. Completeness Table\n",
    "    # Create a pivot table showing which Steps exist for each Model+Dataset\n",
    "    print(\"Generating Completeness Table...\")\n",
    "    completeness = (\n",
    "        df_summary.groupby([\"Dataset\", \"Labels\"])[\"Step\"]\n",
    "        .apply(lambda x: sorted(list(set(x))))\n",
    "        .unstack(fill_value=\"-\")\n",
    "    )\n",
    "\n",
    "    # Style the table\n",
    "    # We can just display it as a dataframe\n",
    "    display(completeness)\n",
    "\n",
    "    display(df_summary.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_metric_per_step(\n",
    "    df, metric, title_prefix, ylabel, kind=\"box\", log_scale=False, share_y=True\n",
    "):\n",
    "    if df.empty or metric not in df.columns:\n",
    "        return\n",
    "\n",
    "    unique_steps = sorted(df[\"Step\"].unique())\n",
    "    num_steps = len(unique_steps)\n",
    "\n",
    "    if num_steps == 0:\n",
    "        return\n",
    "\n",
    "    # 1. Establish global order for consistent colors across subplots\n",
    "    global_hue_order = sorted(df[\"Labels\"].dropna().unique())\n",
    "    global_x_order = sorted(df[\"Dataset\"].dropna().unique())\n",
    "\n",
    "    # 2. Pass share_y argument to subplots\n",
    "    fig, axes = plt.subplots(1, num_steps, figsize=(num_steps * 6, 6), sharey=share_y)\n",
    "\n",
    "    if num_steps == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, step in enumerate(unique_steps):\n",
    "        ax = axes[i]\n",
    "        subset = df[df[\"Step\"] == step].copy()\n",
    "\n",
    "        # Convert metric to numeric\n",
    "        if metric in subset.columns:\n",
    "            subset.loc[:, metric] = subset[metric].apply(\n",
    "                lambda x: x.item() if isinstance(x, torch.Tensor) else x\n",
    "            )\n",
    "            subset.loc[:, metric] = pd.to_numeric(subset[metric], errors=\"coerce\")\n",
    "\n",
    "        # Drop NaNs for plotting\n",
    "        subset_plot = subset.dropna(subset=[metric])\n",
    "\n",
    "        if subset_plot.empty:\n",
    "            ax.set_title(f\"$\\Delta t={step}$ (No Valid Data)\")\n",
    "            ax.set_xticks([])\n",
    "\n",
    "            # If not sharing Y, we still want the label on the empty plot to maintain layout\n",
    "            if i == 0 or not share_y:\n",
    "                ax.set_ylabel(ylabel)\n",
    "            else:\n",
    "                ax.set_yticks([])  # Hide ticks if sharing Y and not the first\n",
    "            continue\n",
    "\n",
    "        if kind == \"box\":\n",
    "            sns.boxplot(\n",
    "                data=subset_plot,\n",
    "                x=\"Dataset\",\n",
    "                y=metric,\n",
    "                hue=\"Labels\",\n",
    "                order=global_x_order,\n",
    "                hue_order=global_hue_order,\n",
    "                showfliers=False,\n",
    "                ax=ax,\n",
    "            )\n",
    "        elif kind == \"bar\":\n",
    "            sns.barplot(\n",
    "                data=subset_plot,\n",
    "                x=\"Dataset\",\n",
    "                y=metric,\n",
    "                hue=\"Labels\",\n",
    "                order=global_x_order,\n",
    "                hue_order=global_hue_order,\n",
    "                estimator=np.median,\n",
    "                errorbar=None,\n",
    "                ax=ax,\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"$\\Delta t={step}$\")\n",
    "        ax.set_xlabel(\"Dataset\")\n",
    "\n",
    "        # --- Logic for Y-Axis Labels based on share_y ---\n",
    "        if share_y:\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(ylabel)\n",
    "            else:\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.tick_params(\n",
    "                    axis=\"y\", labelleft=False\n",
    "                )  # Hide ticks on subsequent plots\n",
    "        else:\n",
    "            # If NOT sharing Y, every plot gets the label and keeps its ticks\n",
    "            ax.set_ylabel(ylabel)\n",
    "\n",
    "        if log_scale:\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "        # Legend handling\n",
    "        if i == 1:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            if handles:\n",
    "                ax.legend(\n",
    "                    handles,\n",
    "                    labels,\n",
    "                    bbox_to_anchor=(1.05, 1),\n",
    "                    loc=\"upper left\",\n",
    "                    borderaxespad=0.0,\n",
    "                )\n",
    "        else:\n",
    "            if ax.get_legend() is not None:\n",
    "                ax.get_legend().remove()\n",
    "\n",
    "        # Table printing\n",
    "        mean_table = subset.groupby([\"Dataset\", \"Labels\"])[metric].mean().unstack()\n",
    "        print(f\"\\n{metric} for step {step}\")\n",
    "        print(mean_table.to_string(na_rep=\"-\", float_format=lambda x: \"%.2g\" % x))\n",
    "\n",
    "    fig.suptitle(title_prefix, fontsize=16, y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Keypoints & Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    for col, title, ylabel in [\n",
    "        (\"num_matches\", \"Number of Matches\", \"Count\"),\n",
    "        (\"num_keypoints\", \"Number of Keypoints (Total)\", \"Count\"),\n",
    "        (\"num_keypoints0\", \"Number of Keypoints (Img0)\", \"Count\"),\n",
    "        (\"num_keypoints1\", \"Number of Keypoints (Img1)\", \"Count\"),\n",
    "        (\"covisible\", \"Covisible Points\", \"Count\"),\n",
    "        (\"covisible_percent\", \"Covisible Percent\", \"%\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            plot_metric_per_step(df_per_pair, col, title, ylabel, kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Match Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    for col, title, ylabel in [\n",
    "        (\"epi_prec@1e-4\", \"Epipolar Precision @ 1e-4\", \"Precision\"),\n",
    "        (\"epi_prec@5e-4\", \"Epipolar Precision @ 5e-4\", \"Precision\"),\n",
    "        (\"epi_prec@1e-3\", \"Epipolar Precision @ 1e-3\", \"Precision\"),\n",
    "        (\"reproj_prec@1px\", \"Reprojection Precision @ 1px\", \"Precision\"),\n",
    "        (\"reproj_prec@3px\", \"Reprojection Precision @ 3px\", \"Precision\"),\n",
    "        (\"reproj_prec@5px\", \"Reprojection Precision @ 5px\", \"Precision\"),\n",
    "        (\"gt_match_recall@3px\", \"GT Match Recall @ 3px\", \"Recall\"),\n",
    "        (\"gt_match_precision@3px\", \"GT Match Precision @ 3px\", \"Precision\"),\n",
    "        (\"mean_matching_score\", \"Mean Matching Score\", \"Score\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            # Ratios often better as boxplots or bars.\n",
    "            plot_metric_per_step(df_per_pair, col, title, ylabel, kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. RANSAC Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    for col, title, ylabel in [\n",
    "        (\"ransac_inl\", \"RANSAC Inliers\", \"Count\"),\n",
    "        (\"ransac_inl%\", \"RANSAC Inlier %\", \"Percent\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            plot_metric_per_step(df_per_pair, col, title, ylabel, kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Relative Pose Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    # Main\n",
    "    for col, title, ylabel in [\n",
    "        (\"rel_pose_error\", \"Relative Pose Error (Max)\", \"Error\"),\n",
    "        (\"rel_pose_t_error\", \"Rel Translation Error\", \"Meters\"),\n",
    "        (\"rel_pose_r_error\", \"Rel Rotation Error\", \"Deg\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            plot_metric_per_step(\n",
    "                df_per_pair, col, title, ylabel, kind=\"bar\"\n",
    "            )  # Median bar\n",
    "    # Per Axis\n",
    "    for col, title, ylabel in [\n",
    "        (\"rel_pose_t_error_x\", \"Rel Trans Error X\", \"Meters\"),\n",
    "        (\"rel_pose_t_error_y\", \"Rel Trans Error Y\", \"Meters\"),\n",
    "        (\"rel_pose_t_error_z\", \"Rel Trans Error Z\", \"Meters\"),\n",
    "        (\"rel_pose_r_error_roll\", \"Rel Rot Error Roll\", \"Deg\"),\n",
    "        (\"rel_pose_r_error_pitch\", \"Rel Rot Error Pitch\", \"Deg\"),\n",
    "        (\"rel_pose_r_error_yaw\", \"Rel Rot Error Yaw\", \"Deg\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            plot_metric_per_step(df_per_pair, col, title, ylabel, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Absolute Localization Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    for col, title, ylabel in [\n",
    "        (\"abs_loc_t_error\", \"Abs Translation Error\", \"Meters\"),\n",
    "        (\"abs_loc_r_error\", \"Abs Rotation Error\", \"Deg\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            plot_metric_per_step(df_per_pair, col, title, ylabel, kind=\"bar\")\n",
    "    for col, title, ylabel in [\n",
    "        (\"abs_loc_t_error_x\", \"Abs Trans Error X\", \"Meters\"),\n",
    "        (\"abs_loc_t_error_y\", \"Abs Trans Error Y\", \"Meters\"),\n",
    "        (\"abs_loc_t_error_z\", \"Abs Trans Error Z\", \"Meters\"),\n",
    "        (\"abs_loc_r_error_roll\", \"Abs Rot Error Roll\", \"Deg\"),\n",
    "        (\"abs_loc_r_error_pitch\", \"Abs Rot Error Pitch\", \"Deg\"),\n",
    "        (\"abs_loc_r_error_yaw\", \"Abs Rot Error Yaw\", \"Deg\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            plot_metric_per_step(df_per_pair, col, title, ylabel, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Localization Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    for col, title, ylabel in [\n",
    "        (\"abs_loc_acc@0.25m_2deg\", \"Loc Accuracy @ 0.25m 2deg\", \"Success Rate (0-1)\"),\n",
    "        (\"abs_loc_acc@0.5m_5deg\", \"Loc Accuracy @ 0.5m 5deg\", \"Success Rate (0-1)\"),\n",
    "        (\"abs_loc_acc@1.0m_10deg\", \"Loc Accuracy @ 1.0m 10deg\", \"Success Rate (0-1)\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            # Bar plot of the mean gives the accuracy %\n",
    "            # If the column is boolean/0-1, 'barplot' with default estimator=mean works perfectly.\n",
    "            # But earlier we set estimator=np.median for errors.\n",
    "            # For accuracy (0/1), we want MEAN (percentage).\n",
    "            # So let's handle that explicitly or use a different kind=\"bar_mean\" logic.\n",
    "            # Or just pass estimator=np.mean to a custom call here loops.\n",
    "\n",
    "            unique_steps = sorted(df_per_pair[\"Step\"].unique())\n",
    "            for step in unique_steps:\n",
    "                subset = df_per_pair[df_per_pair[\"Step\"] == step]\n",
    "                if subset.empty:\n",
    "                    continue\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(\n",
    "                    data=subset,\n",
    "                    x=\"Dataset\",\n",
    "                    y=col,\n",
    "                    hue=\"Labels\",\n",
    "                    estimator=np.mean,\n",
    "                    errorbar=None,\n",
    "                )\n",
    "                plt.title(f\"{title} (Step {step})\")\n",
    "                plt.ylabel(ylabel)\n",
    "                plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "                plt.xticks(rotation=45, ha=\"right\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. Runtime Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    for col, title, ylabel in [\n",
    "        (\"extraction_time\", \"Extraction Time\", \"Seconds\"),\n",
    "        (\"matching_time\", \"Matching Time\", \"Seconds\"),\n",
    "        (\"total_time\", \"Total Time\", \"Seconds\"),\n",
    "    ]:\n",
    "        if col in df_per_pair.columns:\n",
    "            plot_metric_per_step(df_per_pair, col, title, ylabel, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 8. Summary Heatmaps (Detailed Absolute Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_metrics = [\n",
    "    # Main Accuracies\n",
    "    (\"mabs_loc_acc@0.25m_2deg\", \"Loc Acc @ 0.25m 2deg\", \"float\"),\n",
    "    (\"mabs_loc_acc@0.5m_5deg\", \"Loc Acc @ 0.5m 5deg\", \"float\"),\n",
    "    (\"mabs_loc_acc@1.0m_10deg\", \"Loc Acc @ 1.0m 10deg\", \"float\"),\n",
    "    # Absolute Errors\n",
    "    (\"mabs_loc_t_error\", \"Mean Abs Trans Error (m)\", \"float\"),\n",
    "    (\"mabs_loc_r_error\", \"Mean Abs Rot Error (deg)\", \"float\"),\n",
    "    # Specific Axis Errors (if you want deep dive)\n",
    "    (\"mabs_loc_t_error_z\", \"Mean Abs Z Error (m)\", \"float\"),\n",
    "    # Matching\n",
    "    (\"mean_matching_score\", \"Mean Matching Score\", \"float\"),\n",
    "]\n",
    "\n",
    "if not df_summary.empty:\n",
    "    unique_steps = sorted(df_summary[\"Step\"].unique())\n",
    "    for step in unique_steps:\n",
    "        subset_step = df_summary[df_summary[\"Step\"] == step]\n",
    "        if subset_step.empty:\n",
    "            continue\n",
    "\n",
    "        # Group heatmaps into logical blocks for cleaner display\n",
    "\n",
    "        # 1. Accuracy Heatmaps\n",
    "        acc_metrics = [m for m in summary_metrics if \"acc\" in m[0] or \"score\" in m[0]]\n",
    "        for metric, title, fmt in acc_metrics:\n",
    "            if metric not in subset_step.columns:\n",
    "                continue\n",
    "            pivot_df = subset_step.pivot(\n",
    "                index=\"Dataset\", columns=\"Model\", values=metric\n",
    "            )\n",
    "            if pivot_df.empty:\n",
    "                continue\n",
    "\n",
    "            plt.figure(figsize=(10, len(pivot_df) * 0.8 + 2))\n",
    "            sns.heatmap(pivot_df, annot=True, fmt=\".2f\", cmap=\"viridis\", linewidths=0.5)\n",
    "            plt.title(f\"{title} (Step {step})\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # 2. Error Heatmaps (where Lower is Better - separate colormap?)\n",
    "        err_metrics = [m for m in summary_metrics if \"error\" in m[0]]\n",
    "        for metric, title, fmt in err_metrics:\n",
    "            if metric not in subset_step.columns:\n",
    "                continue\n",
    "            pivot_df = subset_step.pivot(\n",
    "                index=\"Dataset\", columns=\"Model\", values=metric\n",
    "            )\n",
    "            if pivot_df.empty:\n",
    "                continue\n",
    "\n",
    "            plt.figure(figsize=(10, len(pivot_df) * 0.8 + 2))\n",
    "            # Use 'viridis_r' (reversed) or 'magma' so brighter/lighter = higher error?\n",
    "            # Usually keep standard but know that Red/Low isn't necessarily bad if using coolwarm.\n",
    "            # Stick to Viridis but remember high value = bad for error.\n",
    "            sns.heatmap(pivot_df, annot=True, fmt=\".3f\", cmap=\"magma\", linewidths=0.5)\n",
    "            plt.title(f\"{title} (Step {step})\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 9. Focused Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 9a. Baseline Performance across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_summary.empty:\n",
    "    # Filter for Baselines only\n",
    "    baselines = df_summary[df_summary[\"Model Type\"] == \"Baseline\"]\n",
    "    if not baselines.empty:\n",
    "        metric = \"mabs_loc_acc@0.25m_2deg\"\n",
    "        if metric in baselines.columns:\n",
    "            plot_metric_per_step(\n",
    "                baselines,\n",
    "                metric,\n",
    "                f\"Baseline Accuracy ({metric})\",\n",
    "                \"Accuracy\",\n",
    "                kind=\"bar\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 9b. Finetuned Performance across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_summary.empty:\n",
    "    # Filter for Finetuned only\n",
    "    finetuned = df_summary[df_summary[\"Model Type\"] == \"Finetuned\"]\n",
    "    if not finetuned.empty:\n",
    "        metric = \"mabs_loc_acc@0.25m_2deg\"\n",
    "        if metric in finetuned.columns:\n",
    "            plot_metric_per_step(\n",
    "                finetuned,\n",
    "                metric,\n",
    "                f\"Finetuned Accuracy ({metric})\",\n",
    "                \"Accuracy\",\n",
    "                kind=\"bar\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 10. Performance Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    datasets = df_per_pair[\"Dataset\"].unique()\n",
    "    steps = df_per_pair[\"Step\"].unique()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for step in steps:\n",
    "            subset = df_per_pair[\n",
    "                (df_per_pair[\"Dataset\"] == dataset) & (df_per_pair[\"Step\"] == step)\n",
    "            ]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            sns.lineplot(\n",
    "                data=subset,\n",
    "                x=\"Frame\",\n",
    "                y=\"rel_pose_r_error\",\n",
    "                hue=\"Labels\",\n",
    "                style=\"Model Type\",\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            plt.title(f\"Rotation Error Trajectory - {dataset} (Step {step})\")\n",
    "            plt.ylabel(\"Rotation Error (deg)\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_pair.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_per_pair.empty:\n",
    "    datasets = df_per_pair[\"Dataset\"].unique()\n",
    "    steps = df_per_pair[\"Step\"].unique()\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for step in steps:\n",
    "            subset = df_per_pair[\n",
    "                (df_per_pair[\"Dataset\"] == dataset) & (df_per_pair[\"Step\"] == step)\n",
    "            ]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            sns.lineplot(\n",
    "                data=subset,\n",
    "                x=\"Frame\",\n",
    "                y=\"abs_loc_r_error\",\n",
    "                hue=\"Labels\",\n",
    "                style=\"Model Type\",\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            plt.title(f\"Rotation Error Trajectory - {dataset} (Step {step})\")\n",
    "            plt.ylabel(\"Rotation Error (deg)\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            sns.lineplot(\n",
    "                data=subset,\n",
    "                x=\"Frame\",\n",
    "                y=\"abs_loc_t_error\",\n",
    "                hue=\"Labels\",\n",
    "                style=\"Model Type\",\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            plt.title(f\"Translation Error Trajectory - {dataset} (Step {step})\")\n",
    "            plt.ylabel(\"Translation Error (m)\")\n",
    "            plt.yscale(\"log\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 11. Per-Dataset Detailed Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_summary.empty:\n",
    "    datasets = sorted(df_summary[\"Dataset\"].unique())\n",
    "    metrics_to_show = [\"mabs_loc_acc@0.25m_2deg\", \"mrel_pose_r_error\"]\n",
    "\n",
    "    for dataset in datasets:\n",
    "        subset = df_summary[df_summary[\"Dataset\"] == dataset]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        # Plot 2 key metrics for this dataset\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        fig.suptitle(f\"Performance on {dataset} Dataset\")\n",
    "\n",
    "        for i, metric in enumerate(metrics_to_show):\n",
    "            if metric in subset.columns:\n",
    "                sns.barplot(\n",
    "                    data=subset,\n",
    "                    x=\"Step\",\n",
    "                    y=metric,\n",
    "                    hue=\"Labels\",\n",
    "                    ax=axes[i],\n",
    "                    errorbar=None,\n",
    "                )\n",
    "                axes[i].set_title(metric)\n",
    "                axes[i].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 12. Speed vs Accuracy Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    not df_summary.empty\n",
    "    and \"mabs_loc_acc@0.25m_2deg\" in df_summary.columns\n",
    "    and \"mmatching_time\" in df_summary.columns\n",
    "):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(\n",
    "        data=df_summary,\n",
    "        x=\"mmatching_time\",\n",
    "        y=\"mabs_loc_acc@0.25m_2deg\",\n",
    "        hue=\"Labels\",\n",
    "        style=\"Dataset\",\n",
    "        s=150,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.title(\"Speed vs Accuracy (Matching Time vs Loc Acc @ 0.25m)\")\n",
    "    plt.xlabel(\"Mean Matching Time (s)\")\n",
    "    plt.ylabel(\"Localization Accuracy (%)\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 5. Evaluation Completeness (Recap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Completeness Table (Recap):\")\n",
    "display(completeness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lupnt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
